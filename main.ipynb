{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "  <div style=\"flex: 1; max-width: 20%; padding-right: 10px;\">\n",
    "    <img src=\"./images/RAGme-logo.png\" width=150px>\n",
    "  </div>\n",
    "  <div style=\"flex: 2; max-width: 80%;\">\n",
    "    <center><h1>RAGme</h1></center>\n",
    "    <p>The RAGme project is an innovative platform designed to enhance information retrieval and response generation. It combines the power of Retrieval-Augmented Generation (RAG) with a robust, vectorized database, allowing users to upload their knowledge base documents which are then transformed into a searchable format. This setup enables the system to efficiently retrieve relevant information in response to user queries.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import all the necessary libraries to launch ChatLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables and API Validation\n",
    "\n",
    "Starts loading environment variables from a `.env` file inside the project folder\n",
    "\n",
    "Create the `.env` file with the following cell, open it and store inside it your API keys:\n",
    "<br>\n",
    "<br>➡️ OPENAI_API_KEY has to be setted with your OpenAI API key created from https://platform.openai.com/account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! [ ! -f .env ] && touch .env && echo \"OPENAI_API_KEY=your_openai_api_key_here\"> .env >> .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables from the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading environment variables...\n",
      "✅ OpenAI API key found and valid.\n"
     ]
    }
   ],
   "source": [
    "print(\"⏳ Loading environment variables...\")\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "# check OpenAI API key\n",
    "if not openai_api_key or not openai_api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"❌ ERROR: OpenAI API key invalid or missing.\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found and valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folders and Model initialization\n",
    "\n",
    "Define the `upload` folder and the `vector_db` folder where the input files and the corresponding vectorized database will take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload directory already exists.\n",
      "✅ Vector database directory already exists.\n"
     ]
    }
   ],
   "source": [
    "# directory where uploaded files will be stored\n",
    "UPLOAD_DIR = \"upload\"\n",
    "VECTOR_DB_PATH = \"vector_db\"\n",
    "\n",
    "# create the upload directory if it doesn't exist\n",
    "if not os.path.exists(UPLOAD_DIR):\n",
    "    os.makedirs(UPLOAD_DIR)\n",
    "    print(\"✅ Upload directory created.\")\n",
    "else:\n",
    "    print(\"✅ Upload directory already exists.\")\n",
    "\n",
    "\n",
    "# create the vector database directory if it doesn't exist\n",
    "if not os.path.exists(VECTOR_DB_PATH):\n",
    "    os.makedirs(VECTOR_DB_PATH)\n",
    "    print(\"✅ Vector database directory created.\")\n",
    "else:\n",
    "    print(\"✅ Vector database directory already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model choosed: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "print(f\"✅ Model choosed: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the database\n",
    "\n",
    "Create the vectorized database with FAISS standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process all files in UPLOAD_DIR and create a vectorized database\n",
    "def process_all_files():\n",
    "    documents = []\n",
    "\n",
    "    if os.listdir(UPLOAD_DIR) == []:\n",
    "        return f\"❌ Missing files in the upload directory\"\n",
    "    \n",
    "    for file_name in os.listdir(UPLOAD_DIR):\n",
    "        file_path = os.path.join(UPLOAD_DIR, file_name)\n",
    "        \n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            continue  # skip unsupported file types\n",
    "        \n",
    "        documents.extend(loader.load())\n",
    "    \n",
    "    # split documents into smaller chunks\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # create vectorized embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    vectorstore.save_local(VECTOR_DB_PATH)\n",
    "\n",
    "    return f\"✅ Processed {len(chunks)} document chunks and stored in FAISS database.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inizialize the RAG pipleine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize the RAG model\n",
    "def initialize_rag():\n",
    "    if not os.path.exists(VECTOR_DB_PATH):\n",
    "        return \"❌ No vector database found. Upload and process files first.\"\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    llm = ChatOpenAI(temperature=0.7, model_name=MODEL, streaming=True)\n",
    "    \n",
    "    return ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to handle chat queries\n",
    "def chat_rag(query, history):\n",
    "    global conversation_chain\n",
    "    if conversation_chain is None:\n",
    "        conversation_chain = initialize_rag()\n",
    "        if isinstance(conversation_chain, str):\n",
    "            return conversation_chain  # return error message if vector DB is missing\n",
    "    \n",
    "    result = conversation_chain.invoke({\"question\": query})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch the application\n",
    "\n",
    "Launch the gradio UI interface on the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliorusso/miniconda3/envs/llm/lib/python3.13/site-packages/gradio/components/chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7903\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7903/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the Gradio interface\n",
    "with gr.Blocks() as app:\n",
    "    # embed the chat interface inside the Blocks UI\n",
    "    chat_ui = gr.ChatInterface(chat_rag, title=\"RAGme\", description=\"1. Move your knowledge base documents in `upload`\\n2. Click on `Build Knowledge Base`\\n3. Ask to GPT\")\n",
    "\n",
    "    # build the vector database\n",
    "    process_button = gr.Button(\"Build Knowledge Base\")\n",
    "    process_output = gr.Textbox(label=\"Status:\", placeholder=\"Load your documents in the upload directory and click the 'Build Knowledge Base' button.\")\n",
    "    process_button.click(process_all_files, outputs=[process_output])\n",
    "\n",
    "# launch the Gradio app\n",
    "app.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
